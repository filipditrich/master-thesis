%%% Data and Methodology
%%%%%%% Wording: ⏳
%%%%%%% Styling: ⏳
%%%%%%% References: ⏳
%%%%% Grammar: ⏳
%%% --------------------------------------------------------------
\chapter{Data and Methodology}
\label{ch:data-methodology}
This chapter address the process and challenges of local environment setup, obtaining, preparing and anonymizing the data.
Most importantly, this chapter describes and explains the data that was used for this research.
It also briefly describes the tools, technologies and methods employed to answer the research questions.


\section{Environment and local setup}
\label{sec:data-methodology-environment}
\todo{PostgreSQL, Python, DataSpell, ...}

To start off, we needed to set up some kind of environment where we would later work with the data.
The data we would be working with was stored in a PostgreSQL database.

Having direct access to the production database to perform the analysis was not a secure and ethical way to go.
Exporting only the necessary and raw data from the production database was an initial thought, but we initially did not know what data we would need, and by exporting we would lose all the relations between the tables.

Therefore, we decided to set up a local database with the same structure as the production database where we can query and analyze the data safely.
The next step was to import the data from the production database to the local database.
Importing or simple cloning the full database was also not an option, because only a small fraction of its subset was required.

So a deep internal analysis of the tables that were relevant to our study was performed.
This resulted in a list of total 21 tables that held the necessary data for the study and were necessary to be imported.

\subsection{Data Obtaining and Preparation}
\label{subsec:data-methodology-obtaining-preparation}

Almost every table was easily queried for the event and exported from the production database to a local CSV file.
But some tables (for example and not surprisingly, the \textit{transactions} table with over 140k rows) were too large to be exported in one piece, so we had to split them into smaller parts.
Later, these parts were joined together to a single CSV file using a simple Python script.

Since no direct access to the production database was used for the export but rather a database management tool, the export was not as fast as it could be and took a significant amount of time.
Moreover, the exported data, most importantly the timestamps, were in a different format than we needed.
And also all numeric values were exported as a formatted strings with a comma as a decimal separator.
So a data preprocessing Python script was written to convert such invalid columns to the correct format.

\subsection{Local Database Setup}
\label{subsec:data-methodology-local-database-setup}
Then a step to set up the local PostgreSQL database was needed.
Due to the nature of this study, we wanted to keep the setup as simple as possible, so we used the default PostgreSQL installation without using any special environment using Docker or similar.
However, during this process I made a mistake and forgot that a PostgreSQL with PostGIS extension was needed.
So it required to re-set up the database with the PostGIS extension.

The next step was to import the data from the CSV files to the local database.
For further database handling, analysis and visualization, we used DataSpell, a Python IDE with a built-in database explorer and data visualization tools.
DataSpell was then used for the local database import, which prior to it required some necessary database relations and constraints modifications, since the data was exported without them and was not relevant for the study.

This whole process resulted in approximately 387k rows of data in the local database that were ready to be queried and analyzed.

\subsection{Local Database Modifications}
\label{subsec:data-methodology-local-database-modifications}
Before any analysis was performed, some modifications to the local database were needed due to some known limitations and missing data.

\subsubsection{Beverage Volumes}
\label{subsubsec:data-methodology-local-database-modifications-volume}
The first necessary limitation that the Beverage Consumption Analysis section heavily relied on was the missing information about beverage products volume in milliliters.
This information was crucial for the analysis, so a new column was added to the relevant product information tables.
However, the next step was to back-fill this information which was not easily automated.

The First approach was to write a Python script that would try to find the volume information from the product name.
This worked for some products, but not for all since the naming convention was not consistent.

After several attempts to automate this process, it was decided to manually fill in the missing information since only 425 products were present in the database.
Only 159 of them were of beverage type and thus eligible for the volume information.

\subsubsection{Depositable Products}
\label{subsubsec:data-methodology-local-database-modifications-depositable}
Since one of the research questions was to analyze the depositable cups and this information was not easily available in the database, a new column was added to the product information tables.

This was a simple binary column that indicated whether the product was depositable or not.
Back-filling this information was also pretty straightforward since only one product was a depositable cup.

\subsubsection{Venue Map Visualization}
\label{subsubsec:data-methodology-local-database-modifications-venue-map}
One of the initial ideas was to visualize the venue map with the locations of the selling places, top-up service points, stages and other important places.

This would be invaluable, the database was partially ready for this, but the data would be significantly time-consuming to back-fill and the later analysis and visualization would require more time.

Since these facts and the fast that this process of preparing the data took place before completing the list of data analysis questions, this idea was later abandoned.

\subsubsection{Event Program}
\label{subsubsec:data-methodology-local-database-modifications-program}
To present some time-related data and its correlation with the event program, it would require to have the event program in the database.

Again, the database was ready for this, but no event program was set up, since it was unnecessary for the event.
Therefore, this required getting the event program from the festival website and manually insert it into the database.

This was manually a very time-consuming process, but it was necessary for the analysis.
For some simplification of the process, an AI tool was used to extract the data from the program schedule screenshots and instructed to prepare an SQL script that would insert the data into the database.

This seemed like a good idea, but the AI tool was initially hallucinating and made up some incorrect data.
But after several iterations, it successfully extracted the data and prepared the SQL script which was used and the event program was successfully inserted into the database.

In the end, I doubt that this process was faster than manual data entry, but it was a good exercise and a good example of how AI can be used to automate some processes.


\section{Data Anonymization}
\label{sec:data-methodology-anonymization}
The Data Anonymization process was necessary due to requirements initially set by the data provider and later by the ethical considerations.
This step was performed for the already imported data in the local database.
It required identifying the sensitive data and replacing them with anonymized values.

In this case, the most sensitive data were:
\begin{itemize}
	\item \textbf{Vendor names}: Since it included the legal names of the vendors, it was necessary to anonymize them.
	\item \textbf{Selling places}: Some selling places were named after the vendors, so it was necessary to anonymize them as well.
	\item \textbf{Customer information}: Some tables included customer information like names, emails, phone numbers, etc.
\end{itemize}

The process could have been done various ways, but the fact that this study will not be exposing internal database structure, it was decided to perform the anonymization directly in the database.

However, if one-way anonymization were to be performed, it would permanently overwrite the original data, losing the possibility to switch from anonymized to original data.
Therefore, a two-way anonymization process was chosen and performed.

This was particularly useful during the analysis phase, where the results would contain the original data for better understanding, fact-checking and for the internal presentation and consultations with the organizer.

It was done on the database level, where two new internal tables were introduced~–~\textit{public.anonymization\_config} and \textit{public.original\_values}.

Where the \textit{public.anonymization\_config} table held the configuration about which schema, table and column should be anonymized and how.
For the usage, a simple SQL function was created to define the anonymization configuration in a simple JSON format, that looked like in the~\fullref{lst:anonymization-configuration}.

\begin{listing}[H]
	\begin{minted}{sql}
		SELECT configure_anonymization('[
			{ "table": "schema.seller", "columns": ["legal_name", "name_int", "name_pub"] },
			...
			{ "table": "schema.user_account", "columns": ["email", "last_name", "first_name", "phone"] },
			]'::JSONB
		);
	\end{minted}
	\caption{Anonymization configuration example.}
	\label{lst:anonymization-configuration}
\end{listing}

The \textit{public.original\_values} table was used to store the original values of the anonymized columns.
Again, using a simple SQL function \textit{anonymize\_database()}, it would store the original values and anonymize the configured table columns.

One particular challenge was anonymizing the values smartly.
It could have been easily done by replacing the values with random strings, hashes or encrypted values.
But working with data, where a vendor is named \textit{fa65165b923e9cc} is not very convenient.

Therefore, a simple SQL function was written to anonymize the value depending on the configuration.
This allowed to configure the anonymization to:
\begin{itemize}
	\item replace vendor names with values like \textit{Vendor 1},
	\item customer emails with \textit{03b09592-d0eb-43a3-9941-30d38ade6bce@gmail.com} keeping the original email domain,
	\item selling places with values like \textit{Place 1} where the original name contained sensitive information, but keep original values for places like \textit{BAR L2},~etc.
\end{itemize}

In the end, it resulted in a database with anonymized values per stated configuration, that could have been used for the analysis and results presentation safely.
However, the original values were still present in the database, and the database could have been anytime easily restored to the original state if needed and vice versa.


\section{Data Sources}
\label{sec:data-methodology-sources}
\todo{Describe the data sources, with what data we are working with.}


\section{Conclusion}
\label{sec:data-methodology-conclusion}
\todo{Conclude.}

